{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ba951d",
   "metadata": {},
   "source": [
    "## Step1~2. Data load 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881b5c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce198223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('./data/ChatbotData.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99d1f417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고유값: [0 1 2]\n",
      "고유값의 개수: 3\n"
     ]
    }
   ],
   "source": [
    "# label 고유값과 개수 확인\n",
    "unique_labels = train_data['label'].unique()  # 고유값 추출\n",
    "num_unique_labels = len(unique_labels)       # 고유값 개수 계산\n",
    "\n",
    "print(\"고유값:\", unique_labels)\n",
    "print(\"고유값의 개수:\", num_unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "004fdc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "챗봇 샘플의 개수 : 11823\n"
     ]
    }
   ],
   "source": [
    "print('챗봇 샘플의 개수 :', len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f41fab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q        0\n",
      "A        0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58afc80",
   "metadata": {},
   "source": [
    "원 데이터에서 ?, ., !와 같은 구두점을 미리 처리해두어야 하는데, 구두점들을 단순히 제거할 수도 있겠지만, 여기서는 구두점 앞에 공백. 즉, 띄어쓰기를 추가하여 다른 문자들과 구분하겠습니다.\n",
    "\n",
    "가령, '12시 땡!' 이라는 문장이 있다면 '12시 땡 !'으로 땡과 !사이에 공백을 추가합니다. 이는 정규 표현식을 사용하여 가능합니다. 이 전처리는 질문 데이터와 답변 데이터 모두에 적용해줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b108061",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "for sentence in train_data['Q']:\n",
    "    # 구두점에 대해서 띄어쓰기\n",
    "    # ex) 12시 땡! -> 12시 땡 !\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    questions.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2823a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for sentence in train_data['A']:\n",
    "    # 구두점에 대해서 띄어쓰기\n",
    "    # ex) 12시 땡! -> 12시 땡 !\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    answers.append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f92f5d2",
   "metadata": {},
   "source": [
    "질문과 대답에 대해서 상위 5개만 출력하여 구두점들이 띄어쓰기를 통해 분리되었는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d252aa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12시 땡 !', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'PPL 심하네']\n",
      "['하루가 또 가네요 .', '위로해 드립니다 .', '여행은 언제나 좋죠 .', '여행은 언제나 좋죠 .', '눈살이 찌푸려지죠 .']\n"
     ]
    }
   ],
   "source": [
    "print(questions[:5])\n",
    "print(answers[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36e2a8f",
   "metadata": {},
   "source": [
    "'하루가 또 가네요 .'와 같이 구두점 앞에 띄어쓰기가 추가되어 분리된 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec22af",
   "metadata": {},
   "source": [
    "## Step 3. SubwordTextEncoder 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee15bde",
   "metadata": {},
   "source": [
    "자주 사용되는 서브워드 단위로 토큰을 분리하는 토크나이저로 학습 데이터로부터 학습하여 서브워드로 구성된 단어 집합을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b66cc878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서브워드텍스트인코더를 사용하여 질문, 답변 데이터로부터 단어 집합(Vocabulary) 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a07a8",
   "metadata": {},
   "source": [
    "단어 집합이 생성되었습니다. 그런데 인코더-디코더 모델 계열에는 디코더의 입력으로 사용할 시작을 의미하는 시작 토큰 SOS와 종료 토큰 EOS 또한 존재합니다. 해당 토큰들도 단어 집합에 포함시킬 필요가 있으므로 이 두 토큰에 정수를 부여해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37c267be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰과 종료 토큰에 대한 정수 부여.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "# 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94808a50",
   "metadata": {},
   "source": [
    "시작 토큰과 종료 토큰을 추가해주었으나 단어 집합의 크기도 +2를 해줍니다.\n",
    "시작 토큰의 번호와 종료 토큰의 번호, 그리고 단어 집합의 크기를 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d9570ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시작 토큰 번호 : [8178]\n",
      "종료 토큰 번호 : [8179]\n",
      "단어 집합의 크기 : 8180\n"
     ]
    }
   ],
   "source": [
    "print('시작 토큰 번호 :',START_TOKEN)\n",
    "print('종료 토큰 번호 :',END_TOKEN)\n",
    "print('단어 집합의 크기 :',VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf188f6",
   "metadata": {},
   "source": [
    "패딩에 사용될 0번 토큰부터 마지막 토큰인 8,179번 토큰까지의 개수를 카운트하면 단어 집합의 크기는 8,180개입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a98f9496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임의의 질문 샘플을 정수 인코딩 : [5766, 611, 3509, 141, 685, 3747, 849]\n"
     ]
    }
   ],
   "source": [
    "# 서브워드텍스트인코더 토크나이저의 .encode()를 사용하여 텍스트 시퀀스를 정수 시퀀스로 변환.\n",
    "print('임의의 질문 샘플을 정수 인코딩 : {}'.format(tokenizer.encode(questions[20])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36348b68",
   "metadata": {},
   "source": [
    "임의의 질문 문장이 정수 시퀀스로 변환되었습니다. 반대로 정수 인코딩 된 결과는 다시 decode()를 사용하여 기존의 텍스트 시퀀스로 복원할 수 있습니다. 20번 질문 샘플을 가지고 정수 인코딩하고, 다시 이를 디코딩하는 과정은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eb83535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 [5766, 611, 3509, 141, 685, 3747, 849]\n",
      "기존 문장: 가스비 비싼데 감기 걸리겠어\n"
     ]
    }
   ],
   "source": [
    "# 서브워드텍스트인코더 토크나이저의 .encode()와 .decode() 테스트해보기\n",
    "# 임의의 입력 문장을 sample_string에 저장\n",
    "sample_string = questions[20]\n",
    "\n",
    "# encode() : 텍스트 시퀀스 --> 정수 시퀀스\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
    "\n",
    "# decode() : 정수 시퀀스 --> 텍스트 시퀀스\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장: {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1a3536",
   "metadata": {},
   "source": [
    "정수 인코딩 된 문장을 .decode()을 하면 자동으로 서브워드들까지 다시 붙여서 기존 단어로 복원해줍니다. 가령, 정수 인코딩 문장을 보면 정수가 7개인데 기존 문장의 띄어쓰기 단위인 어절은 4개밖에 존재하지 않습니다. 이는 '가스비'나 '비싼데'라는 한 어절이 정수 인코딩 후에는 두 개 이상의 정수일 수 있다는 겁니다. 각 정수가 어떤 서브워드로 맵핑되는지 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92cbf733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5766 ----> 가스\n",
      "611 ----> 비 \n",
      "3509 ----> 비싼\n",
      "141 ----> 데 \n",
      "685 ----> 감기 \n",
      "3747 ----> 걸리\n",
      "849 ----> 겠어\n"
     ]
    }
   ],
   "source": [
    "# 각 정수는 각 단어와 어떻게 mapping되는지 병렬로 출력\n",
    "# 서브워드텍스트인코더는 의미있는 단위의 서브워드로 토크나이징한다. 띄어쓰기 단위 X 형태소 분석 단위 X\n",
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12bf0d",
   "metadata": {},
   "source": [
    "샘플 1개를 가지고 정수 인코딩과 디코딩을 수행해보았습니다. 이번에는 전체 데이터에 대해서 정수 인코딩과 패딩을 진행합니다. 이를 위한 함수로 tokenize_and_filter()를 만듭니다. 여기서는 임의로 패딩의 길이는 8로 정했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18d20eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이를 8로 정의\n",
    "MAX_LENGTH = 8\n",
    "\n",
    "# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    tokenized_inputs.append(sentence1)\n",
    "    tokenized_outputs.append(sentence2)\n",
    "\n",
    "  # 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "  return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7dc241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824f9de7",
   "metadata": {},
   "source": [
    "정수 인코딩과 패딩이 진행된 후의 데이터의 크기를 확인해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b773db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 데이터의 크기(shape) : (11823, 8)\n",
      "답변 데이터의 크기(shape) : (11823, 8)\n"
     ]
    }
   ],
   "source": [
    "print('질문 데이터의 크기(shape) :', questions.shape)\n",
    "print('답변 데이터의 크기(shape) :', answers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2f1cf8",
   "metadata": {},
   "source": [
    "질문과 답변 데이터의 모든 문장이 모두 길이 40으로 변환되었습니다. 임의로 0번 샘플을 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7eede36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8178 7915 4207 3060   41 8179    0    0]\n",
      "[8178 3844   74 7894    1 8179    0    0]\n"
     ]
    }
   ],
   "source": [
    "# 0번 샘플을 임의로 출력\n",
    "print(questions[0])\n",
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dcbe62",
   "metadata": {},
   "source": [
    "길이 40을 맞추기 위해 뒤에 0이 패딩된 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10da2d7",
   "metadata": {},
   "source": [
    "tf.data.Dataset을 사용하여 데이터를 배치 단위로 불러올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d923e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로우 dataset을 이용하여 셔플(shuffle)을 수행하되, 배치 크기로 데이터를 묶는다.\n",
    "# 또한 이 과정에서 교사 강요(teacher forcing)을 사용하기 위해서 디코더의 입력과 실제값 시퀀스를 구성한다.\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더의 실제값 시퀀스에서는 시작 토큰을 제거해야 한다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1] # 디코더의 입력. 마지막 패딩 토큰이 제거된다.\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]  # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다.\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b71e5986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8178 3844   74 7894    1 8179    0    0]\n",
      "[[8178 3844   74 7894    1 8179    0]]\n",
      "[[3844   74 7894    1 8179    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 샘플에 대해서 [:, :-1]과 [:, 1:]이 어떤 의미를 가지는지 테스트해본다.\n",
    "print(answers[0]) # 기존 샘플\n",
    "print(answers[:1][:, :-1]) # 마지막 패딩 토큰 제거하면서 길이가 39가 된다.\n",
    "print(answers[:1][:, 1:]) # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다. 길이는 역시 39가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623cd605",
   "metadata": {},
   "source": [
    "## Step 4. 모델 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6441d6",
   "metadata": {},
   "source": [
    "이제 트랜스포머를 만들어봅시다. 하이퍼파라미터를 조정하여 실제 논문의 트랜스포머보다는 작은 모델을 만듭니다.\n",
    "여기서 선택한 주요 하이퍼파라미터의 값은 다음과 같습니다.\n",
    "\n",
    "\"\n",
    "d_model = 256\n",
    "num_layers = 2\n",
    "num_heads = 8\n",
    "d_ff = 512\n",
    "\"\n",
    "\n",
    "(근데 하면서 바꿀게요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d15443e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#포지셔널 인코딩 구현\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    angle_rads = np.zeros(angle_rads.shape)\n",
    "    angle_rads[:, 0::2] = sines\n",
    "    angle_rads[:, 1::2] = cosines\n",
    "    pos_encoding = tf.constant(angle_rads)\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "\n",
    "    print(pos_encoding.shape)\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a448fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#스케일드 닷 프로덕트 어텐션 구현\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "  # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "  # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "  # padding_mask : (batch_size, 1, 1, key의 문장 길이)\n",
    "\n",
    "  # Q와 K의 곱. 어텐션 스코어 행렬.\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 스케일링\n",
    "  # dk의 루트값으로 나눠준다.\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.\n",
    "  # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\n",
    "  # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61482600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#멀티헤드어텐션 구현\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    # d_model을 num_heads로 나눈 값.\n",
    "    # 논문 기준 : 64\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    # WQ, WK, WV에 해당하는 밀집층 정의\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    # WO에 해당하는 밀집층 정의\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  # num_heads 개수만큼 q, k, v를 split하는 함수\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n",
    "    # q : (batch_size, query의 문장 길이, d_model)\n",
    "    # k : (batch_size, key의 문장 길이, d_model)\n",
    "    # v : (batch_size, value의 문장 길이, d_model)\n",
    "    # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # 2. 헤드 나누기\n",
    "    # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "    # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n",
    "    # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "    # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 4. 헤드 연결(concatenate)하기\n",
    "    # (batch_size, query의 문장 길이, d_model)\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 5. WO에 해당하는 밀집층 지나기\n",
    "    # (batch_size, query의 문장 길이, d_model)\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "973d024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#패딩 마스크 구현\n",
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, key의 문장 길이)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d81afbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#인코더 구현\n",
    "def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # 인코더는 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "          'mask': padding_mask # 패딩 마스크 사용\n",
    "      })\n",
    "\n",
    "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)\n",
    "  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f3eced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#인코더 쌓기\n",
    "def encoder(vocab_size, num_layers, dff,\n",
    "            d_model, num_heads, dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 인코더는 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 포지셔널 인코딩 + 드롭아웃\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # 인코더를 num_layers개 쌓기\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "        dropout=dropout, name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfe3e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더의 첫번째 서브층(sublayer)에서 미래 토큰을 Mask하는 함수\n",
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x) # 패딩 마스크도 포함\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8cc6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#디코더 구현\n",
    "def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "\n",
    "  # 룩어헤드 마스크(첫번째 서브층)\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "\n",
    "  # 패딩 마스크(두번째 서브층)\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "          'mask': look_ahead_mask # 룩어헤드 마스크\n",
    "      })\n",
    "\n",
    "  # 잔차 연결과 층 정규화\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V\n",
    "          'mask': padding_mask # 패딩 마스크\n",
    "      })\n",
    "\n",
    "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\n",
    "  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d01bceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#디코더 쌓기\n",
    "def decoder(vocab_size, num_layers, dff,\n",
    "            d_model, num_heads, dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "\n",
    "  # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 포지셔널 인코딩 + 드롭아웃\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # 디코더를 num_layers개 쌓기\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "        dropout=dropout, name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd2a613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#트랜스포머 구현\n",
    "def transformer(vocab_size, num_layers, dff,\n",
    "                d_model, num_heads, dropout,\n",
    "                name=\"transformer\"):\n",
    "\n",
    "  # 인코더의 입력\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 디코더의 입력\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더의 패딩 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더의 룩어헤드 마스크(첫번째 서브층)\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask, output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 디코더의 패딩 마스크(두번째 서브층)\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더의 출력은 enc_outputs. 디코더로 전달된다.\n",
    "  enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "      d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크\n",
    "\n",
    "  # 디코더의 출력은 dec_outputs. 출력층으로 전달된다.\n",
    "  dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "      d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 다음 단어 예측을 위한 출력층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e05d31ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8180, 256)\n",
      "(1, 8180, 256)\n"
     ]
    }
   ],
   "source": [
    "# 트랜스포머 모델에 하이퍼파라미터 적용\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "D_MODEL = 256\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 8\n",
    "DFF = 512\n",
    "DROPOUT = 0.3\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dff=DFF,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25be5728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#손실함수 정의\n",
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c42f6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1f8fe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습률과 옵티마이저 정의 및 모델 컴파일\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  # 레이블의 크기는 (batch_size, MAX_LENGTH - 1)\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0030dd4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "185/185 [==============================] - 11s 25ms/step - loss: 7.0087 - accuracy: 0.1277\n",
      "Epoch 2/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 5.5681 - accuracy: 0.2527\n",
      "Epoch 3/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 4.6188 - accuracy: 0.2791\n",
      "Epoch 4/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 4.2634 - accuracy: 0.2926\n",
      "Epoch 5/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 4.0305 - accuracy: 0.3093\n",
      "Epoch 6/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 3.8228 - accuracy: 0.3211\n",
      "Epoch 7/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 3.6078 - accuracy: 0.3355\n",
      "Epoch 8/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 3.3750 - accuracy: 0.3536\n",
      "Epoch 9/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 3.1298 - accuracy: 0.3761\n",
      "Epoch 10/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 2.8605 - accuracy: 0.4044\n",
      "Epoch 11/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 2.5818 - accuracy: 0.4355\n",
      "Epoch 12/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 2.3082 - accuracy: 0.4692\n",
      "Epoch 13/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 2.0432 - accuracy: 0.5002\n",
      "Epoch 14/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 1.7959 - accuracy: 0.5321\n",
      "Epoch 15/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 1.5717 - accuracy: 0.5610\n",
      "Epoch 16/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 1.3645 - accuracy: 0.5893\n",
      "Epoch 17/80\n",
      "185/185 [==============================] - 5s 26ms/step - loss: 1.1844 - accuracy: 0.6129\n",
      "Epoch 18/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 1.0199 - accuracy: 0.6374\n",
      "Epoch 19/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.8782 - accuracy: 0.6622\n",
      "Epoch 20/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.7567 - accuracy: 0.6801\n",
      "Epoch 21/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.6549 - accuracy: 0.6986\n",
      "Epoch 22/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.5760 - accuracy: 0.7118\n",
      "Epoch 23/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.5029 - accuracy: 0.7262\n",
      "Epoch 24/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.4339 - accuracy: 0.7411\n",
      "Epoch 25/80\n",
      "185/185 [==============================] - 5s 24ms/step - loss: 0.3670 - accuracy: 0.7546\n",
      "Epoch 26/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.3263 - accuracy: 0.7650\n",
      "Epoch 27/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.2869 - accuracy: 0.7725\n",
      "Epoch 28/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.2572 - accuracy: 0.7809\n",
      "Epoch 29/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.2307 - accuracy: 0.7874\n",
      "Epoch 30/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.2109 - accuracy: 0.7928\n",
      "Epoch 31/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.1911 - accuracy: 0.7974\n",
      "Epoch 32/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.1772 - accuracy: 0.8014\n",
      "Epoch 33/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.1631 - accuracy: 0.8045\n",
      "Epoch 34/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.1520 - accuracy: 0.8072\n",
      "Epoch 35/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.1401 - accuracy: 0.8110\n",
      "Epoch 36/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.1349 - accuracy: 0.8121\n",
      "Epoch 37/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.1245 - accuracy: 0.8143\n",
      "Epoch 38/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.1165 - accuracy: 0.8165\n",
      "Epoch 39/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.1118 - accuracy: 0.8184\n",
      "Epoch 40/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.1052 - accuracy: 0.8201\n",
      "Epoch 41/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0971 - accuracy: 0.8215\n",
      "Epoch 42/80\n",
      "185/185 [==============================] - 4s 24ms/step - loss: 0.0961 - accuracy: 0.8224\n",
      "Epoch 43/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0917 - accuracy: 0.8233\n",
      "Epoch 44/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0858 - accuracy: 0.8247\n",
      "Epoch 45/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0858 - accuracy: 0.8251\n",
      "Epoch 46/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0784 - accuracy: 0.8269\n",
      "Epoch 47/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0767 - accuracy: 0.8278\n",
      "Epoch 48/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0755 - accuracy: 0.8275\n",
      "Epoch 49/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0754 - accuracy: 0.8275\n",
      "Epoch 50/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0682 - accuracy: 0.8298\n",
      "Epoch 51/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0661 - accuracy: 0.8304\n",
      "Epoch 52/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0633 - accuracy: 0.8315\n",
      "Epoch 53/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0619 - accuracy: 0.8317\n",
      "Epoch 54/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0592 - accuracy: 0.8324\n",
      "Epoch 55/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0582 - accuracy: 0.8325\n",
      "Epoch 56/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0566 - accuracy: 0.8327\n",
      "Epoch 57/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0536 - accuracy: 0.8338\n",
      "Epoch 58/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0530 - accuracy: 0.8341\n",
      "Epoch 59/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0509 - accuracy: 0.8344\n",
      "Epoch 60/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0507 - accuracy: 0.8342\n",
      "Epoch 61/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0486 - accuracy: 0.8352\n",
      "Epoch 62/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0486 - accuracy: 0.8350\n",
      "Epoch 63/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0449 - accuracy: 0.8361\n",
      "Epoch 64/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0434 - accuracy: 0.8366\n",
      "Epoch 65/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0463 - accuracy: 0.8360\n",
      "Epoch 66/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0435 - accuracy: 0.8364\n",
      "Epoch 67/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0436 - accuracy: 0.8366\n",
      "Epoch 68/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0396 - accuracy: 0.8374\n",
      "Epoch 69/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0396 - accuracy: 0.8376\n",
      "Epoch 70/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0399 - accuracy: 0.8374\n",
      "Epoch 71/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0391 - accuracy: 0.8374\n",
      "Epoch 72/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0360 - accuracy: 0.8388\n",
      "Epoch 73/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0371 - accuracy: 0.8383\n",
      "Epoch 74/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0350 - accuracy: 0.8389\n",
      "Epoch 75/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0352 - accuracy: 0.8385\n",
      "Epoch 76/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0358 - accuracy: 0.8381\n",
      "Epoch 77/80\n",
      "185/185 [==============================] - 5s 24ms/step - loss: 0.0361 - accuracy: 0.8385\n",
      "Epoch 78/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0329 - accuracy: 0.8390\n",
      "Epoch 79/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0315 - accuracy: 0.8397\n",
      "Epoch 80/80\n",
      "185/185 [==============================] - 5s 25ms/step - loss: 0.0315 - accuracy: 0.8394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ad7acd89700>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 80\n",
    "model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2350077f",
   "metadata": {},
   "source": [
    "## Step 5. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afd5aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "  # 단어와 구두점 사이에 공백 추가.\n",
    "  # ex) 12시 땡! -> 12시 땡 !\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad78b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  # 입력 문장에 대한 전처리\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # 입력 문장에 시작 토큰과 종료 토큰을 추가\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 예측 시작\n",
    "  for i in range(MAX_LENGTH):\n",
    "    predictions = model(inputs=[sentence, output], training=False)\n",
    "\n",
    "    # 현재 시점의 예측 단어를 받아온다.\n",
    "    predictions = predictions[:, -1:, :]\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 현재 시점의 예측 단어를 output(출력)에 연결한다.\n",
    "    # output은 for문의 다음 루프에서 디코더의 입력이 된다.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  # 단어 예측이 모두 끝났다면 output을 리턴.\n",
    "  return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a05d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "  prediction = evaluate(sentence)\n",
    "\n",
    "  # prediction == 디코더가 리턴한 챗봇의 대답에 해당하는 정수 시퀀스\n",
    "  # tokenizer.decode()를 통해 정수 시퀀스를 문자열로 디코딩.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ffa23c",
   "metadata": {},
   "source": [
    "## 임의의 물음으로 모델 작동을 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4894faae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 좋아하는 사람 있어?\n",
      "Output: 잘 모르겠어요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"좋아하는 사람 있어?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51bdbb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 사랑하는 사람 있어?\n",
      "Output: 어서 고백해 보세요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"사랑하는 사람 있어?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aca839a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 너는 보통 어떻게 고백해?\n",
      "Output: 먼저 연락을 해보는 건 어떨까요 ?\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"너는 보통 어떻게 고백해?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b48f064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 고백해봐\n",
      "Output: 먼저 고백하는게 최선의 방법이에요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"고백해봐\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e343877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 어떻게 하면 좋을까?\n",
      "Output: 저도 모르겠어요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"어떻게 하면 좋을까?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dd9ca820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 힘들어\n",
      "Output: 언젠가 다 잊을 거예요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"힘들어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e0130f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 멋져\n",
      "Output: 잘 되길 바랄게요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"멋져\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d034715f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Hello?\n",
      "Output: 당신도 예뻐요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"Hello?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "17e1d987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Oh my god\n",
      "Output: 애교 부려보세요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"Oh my god\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f9e1abd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 너 이상해\n",
      "Output: 저는 위로해드리는 로봇이에요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"너 이상해\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1d12cb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 너 F야?\n",
      "Output: 저는 있다고 믿어요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"너 F야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "64769cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 빨리와\n",
      "Output: 바쁜가봐요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"빨리와\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4fb9b1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 너무 바빠\n",
      "Output: 하나씩 하세요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"너무 바빠\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e397fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 매너 없어\n",
      "Output: 저 주세요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"매너 없어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9474f642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 사탕\n",
      "Output: 몸이 힘들게네요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"사탕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6dafc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 못생겼어\n",
      "Output: 충분히 아름다워요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"못생겼어\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e90616",
   "metadata": {},
   "source": [
    "# 회고\n",
    "---\n",
    "* ## 어제 휴가를 내고 하느라 혼자 하기 많이 힘들었습니다.\n",
    "* ## 그래서 위키독스를 거의 가져와서 그대로 실행해봤습니다.\n",
    "* ## 그걸 기반으로 dropout과 max_length를 조절해서 acc를 올려봤습니다.\n",
    "* ## 신기하게도 max_length가 줄어들 수록 acc가 올랐고, dropout은 0.1, 0.2, 0.3으로 커짐에 따라 acc도 올라갔습니다.\n",
    "* ## 최종적으로 acc를 약 0.84까지 올리기는 했지만, 테스트해보니 그렇게 좋은 성능을 내는지 의문이 들었습니다.\n",
    "* ## 데이터 증강을 사용하거나, 학습률을 조정하고, 배치 크기를 조절하거나, 가중치를 초기화하는 등 다른 하이퍼 파라미터를 수정하면 더 좋은 성능을 내지 않을까 싶습니다.\n",
    "* ## 혹은 정규화 기법을 사용하고, feature engineering 기술이 들어가면 더욱 좋은 모델성능을 보이지 않을까 싶습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66266e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
